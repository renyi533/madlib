/* ----------------------------------------------------------------------- *//**
*
* @file rt.sql_in
*
* @brief Functions used in Rregression tree for data training, prediction and scoring.
*
* @create    September 26, 2012
* @modified  
*
*//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/* Own macro definitions */
m4_ifelse(
    m4_eval(
        m4_ifdef(`__GREENPLUM__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ * 100 + __DBMS_VERSION_MINOR__ < 401
    ), 1,
    `m4_define(`__GREENPLUM_PRE_4_1__')'
)
m4_ifelse(
    m4_eval(
        m4_ifdef(`__POSTGRESQL__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ < 9
    ), 1,
    `m4_define(`__POSTGRESQL_PRE_9_0__')'
)

m4_ifelse(
    m4_eval(
        m4_ifdef(`__GREENPLUM__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ * 10000 +  
        __DBMS_VERSION_MINOR__ * 100 +
        __DBMS_VERSION_PATCH__ >= 40201
    ), 1,
    `m4_define(`__GREENPLUM_GE_4_2_1__')'
)

/**
@addtogroup grp_dectree

@about

This module provides an implementation to grow regression trees.

The implementation supports:
- Building regression tree
- Multiple split critera, including:
  . const_value
  . model(support in future)
- Regression tree Pruning
- Regression tree prediction/scoring
- Regression tree display
- Continuous and discrete features
- Missing value handling

@input

The <b>training data</b> is expected to be of 
the following form:
<pre>{TABLE|VIEW} <em>trainingSource</em> (
    ...
    <em>id</em> INT|BIGINT,
    <em>feature1</em> SUPPORTED_DATA_TYPE,
    <em>feature2</em> SUPPORTED_DATA_TYPE,
    <em>feature3</em> SUPPORTED_DATA_TYPE,
    ....................
    <em>featureN</em> SUPPORTED_DATA_TYPE,
    <em>predict_val</em> SUPPORTED_DATA_TYPE,
    ...
)</pre>

The detailed list of SUPPORTED_DATA_TYPE is: 
SMALLINT, INT, BIGINT, FLOAT8, REAL, 
DECIMAL, INET, CIDR, MACADDR, BOOLEAN,
CHAR, VARCHAR, TEXT, "char", 
DATE, TIME, TIMETZ, TIMESTAMP, TIMESTAMPTZ, and INTERVAL.

The <b>data to predict</b> is expected to be 
of the same form as <b>training data</b>, except
that it does not need a predict_val column.

@usage

- Run the training algorithm on the source data:
  <pre>SELECT * FROM \ref rt_train(
    '<em>rt_type</em>',
    '<em>training_table_name</em>', 
    '<em>result_tree_table_name</em>', 
    '<em>validation_table_name</em>',
    '<em>continuous_feature_names</em>',
    '<em>feature_col_names</em>',
    '<em>id_col_name</em>', 
    '<em>dependent_col_name</em>',
    '<em>min_variance_gain</em>',
    '<em>how2handle_missing_value</em>'
    '<em>max_tree_depth</em>',
    '<em>node_prune_threshold</em>',
    '<em>node_split_threshold</em>'
    '<em>verbosity</em>');
  </pre>
  This will create the regression tree output table storing an abstract object
  (representing the model) used for further prediction. Column names:
  <pre>    
 id | tree_location | fids | coef_vals | feature | is_cont | split_value |    variance_gain     | live | num_of_samples | parent_id | lmc_nid | lmc_fval |    predict_val     
----+---------------+------+-----------+---------+---------+-------------+----------------------+------+----------------+-----------+---------+----------+--------------------
                                                     ...</pre>    
    
- Run the prediction function using the learned model: 
  <pre>SELECT * FROM \ref rt_predict(
    '<em>rt_table_name</em>', 
    '<em>prediction_table_name</em>', 
    '<em>result_table_name</em>'
    '<em>verbosity</em>');</pre>
  This will create the result_table with the prediction results. 
  <pre> </pre> 

- Run the scoring function to score the learned model against a validation data set:
  <pre>SELECT * FROM \ref rt_score(
    '<em>tree_table_name</em>',
    '<em>validation_table_name</em>',
	'<em>verbosity</em>');</pre>
  This will give a root relative squared error value.
  <pre> </pre>

- Run the display tree function using the learned model: 
  <pre>SELECT * FROM \ref rt_display(
    '<em>tree_table_name</em>');</pre>
  This will display the trained regression tree in human readable format. 
  <pre> </pre> 

- Run the clean tree function as below: 
  <pre>SELECT * FROM \ref c45_clean(
    '<em>tree_table_name</em>');</pre>
  This will clean up the learned model and all metadata.
  <pre> </pre> 

@examp

The regression example data comes from a well-known dataset called golf_data
which is commonly used in classification. We transform the class label from 
discrete label to continuous value by the following sql:
-# Prepare an input table/view, e.g.:
\verbatim
sql> create table golf_data_rt as
     select id, outlook, temperature, humidity, windy, 
            case when(class like '%Do not%') 
                 then 0+id/100.0 
                 else 1-id/100.0 
            end as predict_val 
     from golf_data;
sql> select * from golf_data order by id;
 id | outlook  | temperature | humidity | windy  |    class     
----+----------+-------------+----------+--------+--------------
  1 | sunny    |          85 |       85 |  false |  Do not Play
  2 | sunny    |          80 |       90 |  true  |  Do not Play
  3 | overcast |          83 |       78 |  false |  Play
  4 | rain     |          70 |       96 |  false |  Play
  5 | rain     |          68 |       80 |  false |  Play
  6 | rain     |          65 |       70 |  true  |  Do not Play
  7 | overcast |          64 |       65 |  true  |  Play
  8 | sunny    |          72 |       95 |  false |  Do not Play
  9 | sunny    |          69 |       70 |  false |  Play
 10 | rain     |          75 |       80 |  false |  Play
 11 | sunny    |          75 |       70 |  true  |  Play
 12 | overcast |          72 |       90 |  true  |  Play
 13 | overcast |          81 |       75 |  false |  Play
 14 | rain     |          71 |       80 |  true  |  Do not Play
(14 rows)
sql> create table golf_data_rt as
     select id, outlook, temperature, humidity, windy,
            case when(class like '%Do not%')
                 then 0+id/100.0
                 else 1-id/100.0
            end as predict_val
     from golf_data;
sql> select * from golf_data_rt order by id;
 id | outlook  | temperature | humidity | windy  |      predict_val       
----+----------+-------------+----------+--------+------------------------
  1 | sunny    |          85 |       85 |  false | 0.01000000000000000000
  2 | sunny    |          80 |       90 |  true  | 0.02000000000000000000
  3 | overcast |          83 |       78 |  false | 0.97000000000000000000
  4 | rain     |          70 |       96 |  false | 0.96000000000000000000
  5 | rain     |          68 |       80 |  false | 0.95000000000000000000
  6 | rain     |          65 |       70 |  true  | 0.06000000000000000000
  7 | overcast |          64 |       65 |  true  | 0.93000000000000000000
  8 | sunny    |          72 |       95 |  false | 0.08000000000000000000
  9 | sunny    |          69 |       70 |  false | 0.91000000000000000000
 10 | rain     |          75 |       80 |  false | 0.90000000000000000000
 11 | sunny    |          75 |       70 |  true  | 0.89000000000000000000
 12 | overcast |          72 |       90 |  true  | 0.88000000000000000000
 13 | overcast |          81 |       75 |  false | 0.87000000000000000000
 14 | rain     |          71 |       80 |  true  | 0.14000000000000000000
 (14 rows)
\endverbatim

-# Train the regression tree model, e.g.:
\verbatim
sql> SELECT * FROM MADLIB_SCHEMA.rt_clean('trained_tree_rt');
sql> SELECT * FROM MADLIB_SCHEMA.rt_train(
	   'const_value',                       	-- regression tree type
	   'golf_data_rt',                     		-- input table name
	   'trained_tree_rt',                 		-- result tree name
	   null,                             		-- validation table name
	   'temperature,humidity',           		-- continuous feature names
	   'outlook,temperature,humidity,windy',	-- feature column names
	   'id',                             		-- id column name
	   'predict_val',                     		-- predict value column name
	   0.01,                              		-- min variance gain
	   'explicit',                       		-- missing value preparation
	   5,                                		-- max tree depth
	   0.001,                            		-- min percent node
	   0.001,                            		-- min percent split
	   0);                               		-- verbosity
  num_of_samples | tree_nodes | tree_depth | training_time  
 ----------------+------------+------------+----------------
              14 |          8 |          3 | 00:00:01.36607
(1 row)
\endverbatim

-# Check few rows from the tree model table:
\verbatim
sql> select * from trained_tree_rt order by id;
 id | tree_location | fids | coef_vals | feature | is_cont | split_value |    variance_gain     | live | num_of_samples | parent_id | lmc_nid | lmc_fval |    predict_val     
----+---------------+------+-----------+---------+---------+-------------+----------------------+------+----------------+-----------+---------+----------+--------------------
  1 | {0}           | {0}  | {0}       |       2 | f       |             |     1.75117142857143 |    0 |             14 |         0 |       2 |        1 |  0.612142857142857
  2 | {0,1}         | {0}  | {0}       |       3 | t       |          81 |  0.00492500000000037 |    0 |              4 |         1 |         |          |             0.9125
  3 | {0,2}         | {0}  | {0}       |       4 | f       |             |              0.84276 |    0 |              5 |         1 |       5 |        1 |              0.602
  4 | {0,3}         | {0}  | {0}       |       1 | t       |          70 |              0.89568 |    0 |              5 |         1 |       7 |        1 |              0.382
  5 | {0,2,1}       | {0}  | {0}       |       3 | t       |          70 |   0.0020333333333337 |    0 |              3 |         3 |         |          |  0.936666666666667
  6 | {0,2,2}       | {0}  | {0}       |       1 | t       |          70 |               0.0032 |    0 |              2 |         3 |         |          |                0.1
  7 | {0,3,1}       | {0}  | {0}       |       3 | t       |          69 | 0.000199999999999978 |    0 |              2 |         4 |         |          |                0.9
  8 | {0,3,2}       | {0}  | {0}       |       3 | t       |          72 |  0.00283333333333333 |    0 |              3 |         4 |         |          | 0.0366666666666667
(8 rows)
\endverbatim

-# To display the tree with human readable format:
\verbatim
sql> select MADLIB_SCHEMA.rt_display('trained_tree_rt');
                                    rt_display                                    
----------------------------------------------------------------------------------
Root Node : predict_value(0.612142857143)  num_elements(14)                  
    outlook:  = overcast : predict_value(0.9125)  num_elements(4)            
    outlook:  = rain : predict_value(0.602)  num_elements(5)                 
        windy:  =  false : predict_value(0.936666666667)  num_elements(3)    
        windy:  =  true : predict_value(0.1)  num_elements(2)                
    outlook:  = sunny : predict_value(0.382)  num_elements(5)                
        humidity:  <= 70.0  : predict_value(0.9)  num_elements(2)            
        humidity:  > 70.0  : predict_value(0.0366666666667)  num_elements(3)
(1 row)
\endverbatim

-# To classify data with the learned model:
\verbatim
sql> select * from MADLIB_SCHEMA.rt_predict(
         'trained_tree_rt',         -- name of the trained model
         'golf_data_rt',            -- name of the table containing data to predict
         'prediction_result');      -- name of the output table
 input_set_size |    prediction_time    
----------------+-----------------
             14 | 00:00:00.700015
(1 row)
\endverbatim

-# Check prediction results: 
\verbatim
sql> select t.id,t.outlook,t.temperature,t.humidity,t.windy,t.predict_val as real_val,c.predict_val 
     from prediction_result c,golf_data_rt t where t.id=c.id order by id;
 id | outlook  | temperature | humidity | windy  |        real_val        |    predict_val     
----+----------+-------------+----------+--------+------------------------+--------------------
  1 | sunny    |          85 |       85 |  false | 0.01000000000000000000 | 0.0366666666666667
  2 | sunny    |          80 |       90 |  true  | 0.02000000000000000000 | 0.0366666666666667
  3 | overcast |          83 |       78 |  false | 0.97000000000000000000 |             0.9125
  4 | rain     |          70 |       96 |  false | 0.96000000000000000000 |  0.936666666666667
  5 | rain     |          68 |       80 |  false | 0.95000000000000000000 |  0.936666666666667
  6 | rain     |          65 |       70 |  true  | 0.06000000000000000000 |                0.1
  7 | overcast |          64 |       65 |  true  | 0.93000000000000000000 |             0.9125
  8 | sunny    |          72 |       95 |  false | 0.08000000000000000000 | 0.0366666666666667
  9 | sunny    |          69 |       70 |  false | 0.91000000000000000000 |                0.9
 10 | rain     |          75 |       80 |  false | 0.90000000000000000000 |  0.936666666666667
 11 | sunny    |          75 |       70 |  true  | 0.89000000000000000000 |                0.9
 12 | overcast |          72 |       90 |  true  | 0.88000000000000000000 |             0.9125
 13 | overcast |          81 |       75 |  false | 0.87000000000000000000 |             0.9125
 14 | rain     |          71 |       80 |  true  | 0.14000000000000000000 |                0.1
(14 rows)
\endverbatim

-# Score the data against a validation set:
\verbatim
sql> select * from MADLIB_SCHEMA.rt_score(
        'trained_tree_rt',
        'golf_data_rt',
        0);
    rt_score     
-----------------
0.0789553294487
(1 row)

\endverbatim

-# clean up the tree and metadata: 
\verbatim
sql> select MADLIB_SCHEMA.rt_clean('trained_tree_rt');
 rt_clean 
----------
  
(1 row)
\endverbatim

@literature

http://en.wiktionary.org/wiki/regression_tree

@sa File rt.sql_in documenting the SQL functions.
*/


/*
 * @brief The type is returned by function rt_train.
 *
 * @param num_of_samples    The num of samples in the training set.    
 * @param tree_nodes        The num of the nodes in the trained regression tree. 
 * @param tree_depth        The tree depth of result tree.
 * @param training_time     The total training time.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.rt_train_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.rt_train_result AS
    (
        num_of_samples      INT,
        tree_nodes          INT,
        tree_depth          INT,
        training_time       INTERVAL
    );


/*
 * @brief This function constructs a regression tree based on a training
 *        table stored in a database table, each row of which defines a set
 *        of features and an ID. User should specify one continuous
 *        feature to be predicted. The features are represented as a regular
 *        table record. Features could be either discrete or continuous. The
 *        resulting RT will be kept in a table with a predefined schema. A
 *        validate table, also in the form of a database table, whose schema 
 *        is the same as the training table, could be optionally specified to
 *        prune unnecessary branches in the resultant RT.
 *
 * @param rt_type                       The type of the regression tree to be built,
 *                                      including "const_value" and "model".
 *                                      "const_value" means that the predict value in 
 *                                      the regression tree node is an average of the
 *                                      dependent variable of the data row which
 *                                      belongs to this node.
 *                                      "model" option means that there is a linear
 *                                      model in the leaf instead of a subtree,
 *                                      which is used to predict value.
 * @param training_table_name           The name of the table/view that defines the
 *                                      training table.
 * @param result_tree_table_name        The name of the table where the resulting RT
 *                                      will be stored. User must provid that 
 *                                      parameter. No default value is available.
 * @param validation_table_name         The validatetion table used for pruning
 *                                      tree. Default value is null. If that 
 *                                      parameter is null, we will not prune the
 *                                      tree.
 * @param continuous_feature_names      A comma-separated list of the names of the
 *                                      features whose values are continuous. The
 *                                      default value is null.
 * @param feature_col_names             A comma-separated list of names of the table
 *                                      columns, each of which defines a feature. If
 *                                      it is set to null, we will use all columns
 *                                      as features except for the column of "id"
 *                                      and "predict_val".
 * @param id_col_name                   The name of column containing an ID. The
 *                                      default value is "id".
 * @param dependent_col_name            The name of column containing the feature to
 *                                      be predicted. The default value is
 *                                      "predic_val".
 * @param min_variance_gain             Any split that does not decrease the overall
 *                                      variance by a factor of min_variance_gain is
 *                                      not attempted. The main role of this
 *                                      parameter is to save computing time by
 *                                      pruning off splits that are obviously not
 *                                      worthwhile.
 * @param how2handle_missing_value      The name of routine for preprocessing
 *                                      missing value. Default value is "explicit".
 *                                      The other option is "ignore".
 * @param max_tree_depth                Maximum regression tree depth. Default value
 *                                      is 10.
 * @param node_prune_threshold          Specifies the minimum number of cases
 *                                      required in a child node,expressed as a
 *                                      percentage of the rows in the training data.
 *                                      The default is 0.1%.
 * @param node_split_threshold          Specifies the minimum number of cases
 *                                      required in a node in order for a further
 *                                      split to be poosible. Expressed as a
 *                                      percentage of all the rows in the training
 *                                      data. The default is 1%.
 * @param verbosity                     Specifies whether the algorithm should run
 *                                      in verbose mode. The default value is 0.
 *
 * @return MADLIB_SCHEMA.rt_train_result
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.rt_train
    (
    rt_type                     TEXT,
    training_table_name         TEXT,
    result_tree_table_name      TEXT,
    validation_table_name       TEXT,
    continuous_feature_names    TEXT,
    feature_col_names           TEXT,
    id_col_name                 TEXT,
    dependent_col_name          TEXT,
    min_variance_gain           FLOAT,
    how2handle_missing_value    TEXT,
    max_tree_depth              INT,
    node_prune_threshold        FLOAT,
    node_split_threshold        FLOAT,
    verbosity                   INT
    )   
RETURNS MADLIB_SCHEMA.rt_train_result AS $$
    PythonFunctionBodyOnly(`cart', `rt')             
    return rt.train(schema_madlib, rt_type, training_table_name, result_tree_table_name, 
        validation_table_name, continuous_feature_names, feature_col_names, id_col_name,
        dependent_col_name, min_variance_gain, how2handle_missing_value, max_tree_depth, 
        node_prune_threshold, node_split_threshold, verbosity)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief This function check the accuracy of the trained regression tree
 *        model by evaluating the root relative squared error.
 *        The root relative squared error is relative to what it would have
 *        been if a simple predictor had been used. More specifically, this
 *        simple predictor is just the average of the actual values. Thus,
 *        the relative squared error takes the total squared error and
 *        normalizes it by dividing by the total squared error of the simple
 *        predictor. By taking the square root of the relative squared error
 *        one reduces the error to the same dimensions as the quantity being
 *        predicted.
 *        The rrse(root relative squared error) formula as following:
 *        E = sqrt(sum(pow(predict_i - y_i, 2)) / sum(pow(avg(y_i) - y_i ,2)))
 *
 * @param result_tree_table_name    Specify the trained regression tree table.
 * @param scoring_table_name        Specify the table which is used to check the
 *                                  accuracy of the regression tree.
 * @param verbosity                 > 0 means the algorithm run in a verbose mode.
 *
 * @return The float value of root relative squared error.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.rt_score
    (
    result_tree_table_name      TEXT,
    scoring_table_name          TEXT,
    verbosity                   INT
    )
RETURNS FLOAT8 AS $$
    PythonFunctionBodyOnly(`cart', `rt')
    return rt.score(schema_madlib, result_tree_table_name,scoring_table_name, verbosity)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief This is an internal function for displaying one tree node in human
 *        readable format. It is the step function of aggregation named
 *        __display_tree_aggr.
 *
 * @param state             This variable is used to store the accumulated tree
 *                          display information.
 * @param depth             The depth of this node.
 * @param is_cont           Whether the feature used to split is continuous.
 * @param feat_name         The name of the feature used to split.
 * @param curr_val          The value of the splitting feature for this node.
 * @param split_value       For continuous feature, it specifies the split value.
 *                          Otherwise, it is of no meaning.
 * @param predict_val       The predict value in this node.
 * @param num_of_samples    Total count of samples in this node.
 *
 * @return It returns the text containing the information of human
 *         readable information of a regression tree node.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_node_sfunc_rt
    (
    state           TEXT,
    depth           INT,
    is_cont         BOOLEAN,
    feat_name       TEXT,
    curr_val        TEXT,
    split_value     FLOAT8,
    predict_val     FLOAT8,
    num_of_samples  INT
    )
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `rt')
    return rt.__display_node_sfunc(schema_madlib, state, depth, is_cont,
        feat_name, curr_val, split_value, predict_val, num_of_samples)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief The aggregate function to display the regress tree in human readable format
 *        invokes the aggregate MADLIB_SCHEMA.__display_node_sfunc_rt.
 *
 * @param depth             The depth of this node.
 * @param is_cont           Whether the feature used to split is continuous.
 * @param feat_name         The name of the feature used to split.
 * @param curr_val          The value of the splitting feature for this node.
 * @param split_value       For continuous feature, it specifies the split value.
                            Otherwise, it is of no meaning.
 * @param predict_val       The predict value in this node.
 * @param num_of_samples    Total count of samples in this node.
  
 * @return It returns the text containing the information of human
 *         readable information for regression tree.
 */
DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__display_tree_aggr_rt
    (
    INT,        -- depth
    BOOLEAN,    -- is_cont
    TEXT,       -- feature name
    TEXT,       -- curr_val
    FLOAT8,     -- split_value
    FLOAT8,     -- predict_val
    INT         -- num_of_samples
    ) CASCADE;
CREATE 
m4_ifdef(`__GREENPLUM__', m4_ifdef(`__HAS_ORDERED_AGGREGATES__', `ORDERED'))
AGGREGATE MADLIB_SCHEMA.__display_tree_aggr_rt
    (
    INT,        -- depth
    BOOLEAN,    -- is_cont
    TEXT,       -- feature name
    TEXT,       -- curr_val
    FLOAT8,     -- split_value
    FLOAT8,     -- predict_val
    INT         -- num_of_samples
    )
(
    SFUNC = MADLIB_SCHEMA.__display_node_sfunc_rt,
    STYPE = TEXT
);


/*
 * @brief Display the trees in the regression tree with human readable format.
 *
 * @param rt_table_name     Specify the table name of the regression tree.
 *
 * @return The text of regression tree in human readable format.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.rt_display
    (
    rt_table_name           TEXT
    )
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `rt')   
    return rt.display(schema_madlib, rt_table_name)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief The type of regression tree predict result.
 *
 * @param num_of_samples    The num of prediction samples in the prediction data set.
 * @param prediction_time   The time of prediciton.
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.rt_predict_result CASCADE;
CREATE TYPE MADLIB_SCHEMA.rt_predict_result AS
(
    num_of_samples          BIGINT,
    prediction_time         INTERVAL
);


/*
 * @brief Predict dataset using trained decision tree model.
 *
 * @param rt_table_name             The name of trained regression tree.
 * @param prediction_table_name     The name of the table with the source data
 *                                  to be predicted.
 * @param result_table_name         The name of the table that stores the prediction 
 *                                  result.
 * @param verbosity                 > 0 means this function runs in verbose mode.
 *
 * @return A specified table by user that stores the prediction result. 
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.rt_predict
    (
    rt_table_name           TEXT,
    prediction_table_name   TEXT,
    result_table_name       TEXT,
    verbosity               INT
    )
RETURNS MADLIB_SCHEMA.rt_predict_result AS $$
    PythonFunctionBodyOnly(`cart', `rt')
    return rt.predict(schema_madlib, rt_table_name, prediction_table_name, 
        result_table_name, verbosity)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief This function deletes those tables storing the regression tree,
 *        related metadata and temporary tables.
 *
 * @param rt_tbl_name   The regression tree model table that need to be cleaned.
 *
 * @return Return nothing if clean successfully, otherwise raise an exception.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.rt_clean
    (
    rt_tbl_name             TEXT
    )
RETURNS VOID AS $$
    PythonFunctionBodyOnly(`cart', `rt')
    rt.clean(schema_madlib, rt_tbl_name)
$$ LANGUAGE PLPYTHONU;

