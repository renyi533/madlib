% When using TeXShop on the Mac, let it know the root document. The following must be one of the first 20 lines.
% !TEX root = ../design.tex

\chapter{Low-rank Matrix Factorization}

% Abstract. What is the problem we want to solve?
This module implements "factor model" for representing a sparse matrix using a low-rank approximation \cite{DBLP:conf/icml/SrebroJ03}.
Mathematically, this model seeks to find matrices U and V that, for any given sparse matrix A, minimizes:
\[ ||\boldsymbol A - \boldsymbol UV' ||_2 \]
subject to $rank(\boldsymbol UV') \leq k$, where $||\cdot||_2$ denotes the Frobenius norm and $k \leq rank(\boldsymbol A)$.
If $A$ is $m \times n$, then $U$ will be $m \times k$ and $V$ will be $n \times k$.
This model is not intended to do the full decomposition, or to be used as part of inverse procedure.
This model has been widely used in recommendation systems, such as Netflix \cite{:TheNetflixPrize07}.

\section{Incremental Gradient Descent}

% Background. Why can we solve the problem with incremental gradient?
\subsection{Solving as a Convex Program}
Recent work \cite{DBLP:journals/cacm/CandesR12, DBLP:journals/siamrev/RechtFP10} has demonstrated that the low-rank matrix factorization can be solved as a convex programming problem.
This body of work enables us to solve the problem by using gradient-based line search algorithms. The algorithm and related issues are described

\subsection{Formal Description}
\begin{algorithm}[igd-lmf$(k, A, \alpha)$] \label{alg:igd-lmf}
\alginput{Low-rank constraint $k$, sparse matrix $A$, step size $\alpha$,\\convergence strategy $\mathit{Convergence}$}
\algoutput{Matrices $U$ and $V$}
\algprecond{$i = 0$}
\begin{algorithmic}[1]
	\State $U \set \{ \text{each element is chosen uniformly at random from } (0, 1) \}$
	\State $V \set \{ \text{each element is chosen uniformly at random from } (0, 1) \}$
	\Repeat
		\State $i \set i + 1$
		\State $U_\text{old} \set U$
		\State $V_\text{old} \set V$
		\For{$(x, y, a) \in A$}
			\State $e \set U_x \cdot V_y - a$
			\State $temp \set U_x - \alpha e V_y$
			\State $V_y \set V_y - \alpha e U_x$
			\State $U_x \set temp$
		\EndFor
	\Until{$Convergence(U_\text{old}, V_\text{old}, U, V, i)$}
\end{algorithmic}
\end{algorithm}

\begin{description}
	\item[Runtime] $O(k(|A| + m + n))$.
	\item[Space] Store the $temp$, a $k$-floating-point vector
	\item[Scalability] The outer loop is inherently sequential. The inner loop is data-parallel (provided that the $U$ and $V$ are available on all computation nodes; otherwise model averaging is used).
\end{description}
